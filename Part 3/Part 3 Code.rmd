---
output: pdf_document
---
# Impact of Inflation On Stock Market Performance -- A Canadian Perspective (With Code)
**Although this is not the full analysis for part 3, you may find the notes in this document helpful as supplements to the full analysis.**

## Setup
```{r}
# Import Data
df <- read.csv("Combined Dataset with GDP.csv")
# Check if values are imported correctly
str(df)
```


We can see that there are some NaN values in the dataset, and the percentage values are not in decimal. Because we need access to the first column of "Close" to compute for Monthly Changes in the stock market, we can't delete the NaN values yet, but we can convert the percentage values into decimal.

Additionally, because the Bank of Canada's inflation target upper and lower band didn't change for the period we selected (run the code below to see the graphs), we will remove them as predictors.
```{r}
# par(mfrow=c(1,2))
# hist(df$Inflation_Upper_Band)
# hist(df$Inflation_Lower_Band)
```

### Data Cleaning
```{r}
# Remove Inflation_Upper_ and Lower_Band
df <- df[, !(names(df) %in% c("Inflation_Upper_Band", "Inflation_Lower_Band"))]
# Percentage values are not in decimal form, we will transform them
df$Inflation_by_CPI <- df$Inflation_by_CPI / 100
df$CPI_Common <- df$CPI_Common / 100
df$CPI_Median <- df$CPI_Median / 100
df$CPI_Trim <- df$CPI_Trim / 100

# Make dataset complete case
df <- df[complete.cases(df), ]
```

## Model Fitting
First we will separate the dataset into two -- training set and testing set.
```{r}
# Create a 50/50 split
set.seed(1)

# Opted for a higher number of samples to include in the training dataset
train_df <- df[sample(1:nrow(df), ceiling(nrow(df)/2), replace=FALSE), ]
test_df <- df[which(!(df$Date %in% train_df$Date)), ]

# Delete the Date columnn because we don't need it anymore
train_df <- train_df[, !(names(train_df) %in% c("Date"))]
test_df <- test_df[, !(names(test_df) %in% c("Date"))]

# Confirm the dimensions of the datasets
dim(train_df)
dim(test_df)
```

#### Summary Statistics of Training and Testing Set
```{r}
meanTrain <- apply(train_df, 2, mean)
sdTrain <- apply(train_df, 2, sd)
sdTrain

meanTest <- apply(test_df, 2, mean)
sdTest <- apply(test_df, 2, sd)
```

Variable | mean (s.d.) in Training Set | mean (s.d.) in Testing Set
---------|-----------------------------|---------------------------
`r names(train_df)[1]` | `r round(meanTrain[1], 2)` (`r round(sdTrain[1], 2)`) | `r round(meanTest[1], 2)` (`r round(sdTest[1], 2)`)
`r names(train_df)[2]` | `r round(meanTrain[2], 2)` (`r round(sdTrain[2], 2)`) | `r round(meanTest[2], 2)` (`r round(sdTest[2], 2)`)
`r names(train_df)[3]` | `r round(meanTrain[3], 2)` (`r round(sdTrain[3], 2)`) | `r round(meanTest[3], 2)` (`r round(sdTest[3], 2)`)
`r names(train_df)[4]` | `r round(meanTrain[4], 2)` (`r round(sdTrain[4], 2)`) | `r round(meanTest[4], 2)` (`r round(sdTest[4], 2)`)
`r names(train_df)[5]` | `r round(meanTrain[5], 2)` (`r round(sdTrain[5], 2)`) | `r round(meanTest[5], 2)` (`r round(sdTest[5], 2)`)
`r names(train_df)[6]` | `r round(meanTrain[6], 2)` (`r round(sdTrain[6], 2)`) | `r round(meanTest[6], 2)` (`r round(sdTest[6], 2)`)
`r names(train_df)[7]` | `r round(meanTrain[7], 2)` (`r round(sdTrain[7], 2)`) | `r round(meanTest[7], 2)` (`r round(sdTest[7], 2)`)

***Inflation data provided by the Bank of Canada looks suspiciously good, hmm***

### EDA On Training Set
```{r, fig.width=7, fig.height=5.5}
# Plot Histograms
par(mfrow=c(2,2))
for (i in 1:length(train_df)) {
  hist(
    train_df[,i],
    main=paste("Histogram of", gsub("_", " ", names(train_df)[i])),
    xlab=gsub("_", " ", names(train_df)[i])
    )
}
```

There does appear some skews in the histograms of CPI Common and Real GDP. We will need to watch out for this in our analysis.

```{r, fig.width=7, fig.height=5.5}
# Plot scatterplots against Close
par(mfrow=c(2,2))
for (i in 2:(length(train_df))) {
  plot(
    train_df[,i],
    train_df$Close,
    main=paste(gsub("_", " ", names(train_df)[i]), "vs", "Close"),
    xlab=gsub("_", " ", names(train_df)[i]),
    ylab="Close"
    )
}
```

Scatterplots look good too.

### Specification 1 On Training Dataset
```{r}
s1 <- lm(train_df$Close ~ ., data=train_df)
summary(s1)
```

#### Checking Conditions 1 and 2
```{r, fig.height=5.5, fig.width=7}
# Condition 1
plot(train_df$Close ~ fitted(s1), main="Y versus Y-hat", xlab="Y-hat", ylab="Y")
abline(a=0, b=1)
lines(lowess(train_df$Close ~ fitted(s1)), lty=2)

# Condition 2
pairs(train_df[, c(2:length(train_df))], lower.panel=NULL)
```

Condition 1 looks amazing. Condition 2 also appear to be okay. We will continue to look at the residual plots.

#### Residual Plots
```{r, fig.height=5.5, fig.width=7}
# Get the residuals from spec 1
r1 <- rstandard(s1)

# Plot the residual plots
par(mfrow=c(2,2))
# Residual plot for the fitted values
plot(r1 ~ fitted(s1), xlab="Fitted Values", ylab="Residual", main="Fitted Values vs Residual")
abline(a = 0, b = 0)
# Residual plots for the predictors
for (i in 2:(length(train_df))) {
  plot(r1 ~ train_df[, i], xlab=gsub("_", " ", colnames(train_df)[i]), ylab="Residual", main=paste(gsub("_", " ", colnames(train_df)[i]), "vs Residual"))
  abline(a = 0, b = 0)
}

# Plot the qq plots
qqnorm(r1)
qqline(r1)
```

There are some fanning out pattern appearing in the residual plots for Inflation by CPI, CPI Trim, CPI Median, and CPI Common. We will apply appropriate transformations to correct for that.

Additionally, the Normal Q-Q Plot also seems to be heavy-tailed, we will also need to correct for this as well

### Model Violations Correction
```{r}
# Box-Cox Transformation
library(car)

p <- powerTransform(cbind(
  train_df[, 1],
  train_df[, 2],
  train_df[, 3],
  train_df[, 4],
  train_df[, 5],
  train_df[, 6],
  train_df[, 7]
) ~ 1, family="bcnPower")
summary(p)
```

```{r}
# Apply transformations
transformed_train_df <- cbind(train_df)  # get a copy of the training set
transformed_train_df[, 1] <- transformed_train_df[, 1] * 1
transformed_train_df[, 2] <- log(transformed_train_df[, 2])
transformed_train_df[, 3] <- transformed_train_df[, 3] * 1
transformed_train_df[, 4] <- log(transformed_train_df[, 4])
transformed_train_df[, 5] <- transformed_train_df[, 5] * 1
transformed_train_df[, 6] <- transformed_train_df[, 6] * 1
transformed_train_df[, 7] <- transformed_train_df[, 7] * 1
```

#### Specification 2 On Training Set
```{r}
s2 <- lm(transformed_train_df$Close ~ ., data=transformed_train_df)
summary(s2)
```

#### Checking Conditions 1 and 2
```{r, fig.height=5.5, fig.width=7}
# Condition 1
plot(transformed_train_df$Close ~ fitted(s2), main="Y versus Y-hat", xlab="Y-hat", ylab="Y")
abline(a=0, b=1)
lines(lowess(transformed_train_df$Close ~ fitted(s2)), lty=2)

# Condition 2
pairs(transformed_train_df[, c(2:length(transformed_train_df))], lower.panel=NULL)
```

Condition 1 and 2 still looks good. Although for Volume it looks a bit weird, but no violations.

#### Residual Plots
```{r, fig.height=5.5, fig.width=7}
# Get the residuals from spec 1
r2 <- rstandard(s2)

# Plot the residual plots
par(mfrow=c(2,2))
# Residual plot for the fitted values
plot(r2 ~ fitted(s2), xlab="Fitted Values", ylab="Residual")
abline(a = 0, b = 0)
# Residual plots for the predictors
for (i in 2:(length(transformed_train_df))) {
  plot(r2 ~ transformed_train_df[, i], xlab=gsub("_", " ", colnames(transformed_train_df)[i]), ylab="Residual")
  abline(a = 0, b = 0)
}

# Plot the qq plots
qqnorm(r2)
qqline(r2)
```

As we can see from the above residual plots, the Box-Cox transformation did not provide sufficient improvements, if at all, to the fanning out pattern exhibiting in the different measures of inflation. Thus, we will not be transforming our model and continue to use our first specification.

The slight violation of constant error variance may be due to the recent changes in market sentiment, trading technologies, or other factors that made speculating and investing in the financial markets more accessible or preferable. Thus, our model cannot capture the additional variance introduced recently (approximately 2019 and afterwards).

## Model Selection
Because we have observed that there are slight deviations on either tails in our normal QQ plot, we will raise the cutoff to 0.10 to be more tolerant with our hypothesis tests.

We will first construct a model with the predictors with statistically significant t-test p-values, and perform model diagnostics on it, before we conduct a partial F-test.
```{r}
# Construct the reduced model
reduced_mod <- lm(train_df$Close ~ Volume + CPI_Common + CPI_Median + CPI_Trim + Real_GDP, data=train_df)
summary(reduced_mod)
```

Check conditions 1 and 2
```{r, fig.height=5.5, fig.width=7}
# Condition 1
plot(train_df$Close ~ fitted(reduced_mod), main="Y versus Y-hat", xlab="Y-hat", ylab="Y")
abline(a=0, b=1)
lines(lowess(train_df$Close ~ fitted(reduced_mod)), lty=2)

# Condition 2
pairs(train_df[, c(2, 4:7)], lower.panel=NULL)
```

Plot residual plots and normal QQ plots
```{r, fig.height=5.5, fig.width=7}
# Get the residuals from spec 1
reduced_r <- rstandard(reduced_mod)

# Plot the residual plots
par(mfrow=c(2,2))
# Residual plot for the fitted values
plot(reduced_r ~ fitted(reduced_mod), xlab="Fitted Values", ylab="Residual")
abline(a = 0, b = 0)
# Residual plots for the predictors
for (i in 2:(length(train_df))) {
  plot(r2 ~ train_df[, i], xlab=gsub("_", " ", colnames(train_df)[i]), ylab="Residual")
  abline(a = 0, b = 0)
}

# Plot the qq plots
qqnorm(reduced_r)
qqline(reduced_r)
```

All looks good (excluding slight heteroskedasticity discussed earlier) :)

### Partial F-Test
```{r}
anova(reduced_mod, s1)
```
With a p-value of 0.9957, we fail to reject the null hypothesis, and conclude that there is no evidence suggesting the full model is better than the reduced model. Therefore, we will use the reduced model from now onward.

### Model 1 -- Manually Eliminating Multicollinearity By VIF
As we have seen from our pairs scatterplots, there exist some strong linear relationships between a few predictors. We will now compute the VIF to examine them formally, removing the predictors with VIF > 5 because we want to avoid multicollinearity as our goal is to have a clear picture of the relationship of between the response and the predictors.
```{r}
vif(reduced_mod)
```

Because, by the definition provided on the Bank of Canada website, (not quoted here to avoid plagiarism because I won't provide a reference sheet in this code file), we will eliminate CPI_Trim and CPI_Median and see how it goes.

```{r}
VIF_mod <- lm(train_df$Close ~ Volume + CPI_Common + Real_GDP, data=train_df)
summary(VIF_mod)
vif(VIF_mod)
```

Now, we will check the assumptions for this potential final model
```{r, fig.height=5.5, fig.width=7}
# Condition 1
plot(train_df$Close ~ fitted(VIF_mod), main="Y versus Y-hat", xlab="Y-hat", ylab="Y")
abline(a=0, b=1)
lines(lowess(train_df$Close ~ fitted(VIF_mod)), lty=2)

# Condition 2
pairs(train_df[, c(2, 4, 7)], lower.panel=NULL)

# Get the residuals
VIF_r <- rstandard(VIF_mod)

# Plot the residual plots
par(mfrow=c(2,2))
# Residual plot for the fitted values
plot(VIF_r ~ fitted(VIF_mod), xlab="Fitted Values", ylab="Residual")
abline(a = 0, b = 0)
# Residual plots for the predictors
for (i in c(2, 4, 7)) {
  plot(VIF_r ~ train_df[, i], xlab=gsub("_", " ", colnames(train_df)[i]), ylab="Residual")
  abline(a = 0, b = 0)
}

# Plot the qq plots
qqnorm(VIF_r)
qqline(VIF_r)
```

All good :)

### Model Subsets
#### Through Adjusted R-Squared
```{r}
library(leaps)

best <- regsubsets(Close ~ ., data=train_df, nbest=1)
summary(best)
subsets(best, statistic = "adjr2")
```
The adjusted R-Squared suggest that we implement the full model, which we have already determined that it's not ideal. So we will ignore its suggestion.

#### Through AIC
```{r}
library(MASS)

# Forward selection
stepAIC(
  lm(Close ~ 1, data=train_df),
  scope = list(upper=lm(Close ~ ., data=train_df)),
  direction = "forward",
  k = 2  # We wish to work with AIC
        )
```

From the above result, we can see that if we the model we only gain 0.53 if we add CPI_Trim, and another 1.04 if we add CPI_Median, which are not significant improvements compared with adding Real_GDP, CPI_Common, and Volume. Therefore, we will also ignore the forward step wise AIC.


```{r}
# Backward selection based on AIC
stepAIC(
  lm(Close ~ ., data=train_df),
  scope = list(lower=lm(Close ~ 1, data=train_df)),
  direction = "backward",
  k = 2  # We wish to work with AIC
        )
```

We already knew that CPI_Median and CPI_Trim have high VIF values. Additionally, removing either of them will make the other an insignificant predictor based on the t-test (run below to verify). Therefore, we will also not consider its suggestion and stick with our manual selection.
```{r}
#VIF_mod_1 <- lm(Close ~ Volume + CPI_Median + CPI_Common + Real_GDP, data=train_df)
#VIF_mod_2 <- lm(Close ~ Volume + CPI_Trim + CPI_Common + Real_GDP, data=train_df)

#summary(VIF_mod_1)
#summary(VIF_mod_2)
```

#### Through BIC
```{r}
# Forward Selection
stepAIC(
  lm(Close ~ 1, data=train_df),
  scope = list(upper=lm(Close ~ ., data=train_df)),
  direction = "forward",
  k = log(nrow(train_df))  # We wish to work with BIC
        )
```

Here, it agrees with our manual selection :)

```{r}
# Backward selection based on BIC
stepAIC(
  lm(Close ~ ., data=train_df),
  scope = list(lower=lm(Close ~ 1, data=train_df)),
  direction = "backward",
  k = log(nrow(train_df))  # We wish to work with BIC
        )
```

Same result as forward :)

#### Stepwise Selection
```{r}
# Step wise AIC
stepAIC(lm(Close ~., data=train_df), direction = "both", k = 2)
```

Again, the same as forward and backward AIC, we will ignore it here.

```{r}
# Step wise BIC
stepAIC(lm(Close ~., data=train_df), direction = "both", k = log(nrow(train_df)))
```

Same as before :)

Thus, based on our model selection procedures, we find that the model Close ~ Volume + CPI_Common + Real_GDP is the "best" model.

## Model Validation
```{r}
final_mod <- lm(Close ~ Volume + CPI_Common + Real_GDP, data=train_df)
test_mod <- lm(Close ~ Volume + CPI_Common + Real_GDP, data=test_df)
```

### Multicollinearity

Compare the VIF values
```{r}
vif(final_mod)
vif(test_mod)
```
Everything looks good :)

### Leverage/Influential Points and Outliers

Compare outliers, influential/leverage points
```{r, fig.height=5, fig.width=8}
# For Training Model
par(mfrow=c(2,3), oma=c(0,0,3,0))
# Leverage Points
l_pts_train <- which(hatvalues(final_mod) > 2 * (length(coef(final_mod))-1 + 1) / nrow(train_df))
l_pts_test <- which(hatvalues(test_mod) > 2 * (length(coef(test_mod))-1 + 1) / nrow(test_df))
# Plot in graph
# Training Set
plot(train_df$Close ~ train_df$Volume, main="Volume vs Close", xlab="Volume", ylab="Close")
points(train_df$Close[l_pts_train] ~ train_df$Volume[l_pts_train], col="red", pch=19)
plot(train_df$Close ~ train_df$CPI_Common, main="CPI Common vs Close", xlab="CPI Common", ylab="Close")
points(train_df$Close[l_pts_train] ~ train_df$CPI_Common[l_pts_train], col="red", pch=19)
plot(train_df$Close ~ train_df$Real_GDP, main="Real GDP vs Close", xlab="Real GDP", ylab="Close")
points(train_df$Close[l_pts_train] ~ train_df$Real_GDP[l_pts_train], col="red", pch=19)
# Testing Set
plot(test_df$Close ~ test_df$Volume, main="Volume vs Close", xlab="Volume", ylab="Close")
points(test_df$Close[l_pts_test] ~ test_df$Volume[l_pts_test], col="red", pch=19)
plot(test_df$Close ~ test_df$CPI_Common, main="CPI Common vs Close", xlab="CPI Common", ylab="Close")
points(test_df$Close[l_pts_test] ~ test_df$CPI_Common[l_pts_test], col="red", pch=19)
plot(test_df$Close ~ test_df$Real_GDP, main="Real GDP vs Close", xlab="Real GDP", ylab="Close")
points(test_df$Close[l_pts_test] ~ test_df$Real_GDP[l_pts_test], col="red", pch=19)
mtext("Leverage Points", outer=TRUE, cex=1.2)

# Outliers
train_r <- rstandard(final_mod)
outliers_train <- which(train_r < -4 | train_r > 4)
test_r <- rstandard(test_mod)
outliers_test <- which(test_r < -4 | test_r > 4)
# Plot in graph
# Training Set
plot(train_df$Close ~ train_df$Volume, main="Volume vs Close", xlab="Volume", ylab="Close")
points(train_df$Close[outliers_train] ~ train_df$Volume[outliers_train], col="blue", pch=19)
plot(train_df$Close ~ train_df$CPI_Common, main="CPI Common vs Close", xlab="CPI Common", ylab="Close")
points(train_df$Close[outliers_train] ~ train_df$CPI_Common[outliers_train], col="blue", pch=19)
plot(train_df$Close ~ train_df$Real_GDP, main="Real GDP vs Close", xlab="Real GDP", ylab="Close")
points(train_df$Close[outliers_train] ~ train_df$Real_GDP[outliers_train], col="blue", pch=19)
# Testing Set
plot(test_df$Close ~ test_df$Volume, main="Volume vs Close", xlab="Volume", ylab="Close")
points(test_df$Close[outliers_test] ~ test_df$Volume[outliers_test], col="blue", pch=19)
plot(test_df$Close ~ test_df$CPI_Common, main="CPI Common vs Close", xlab="CPI Common", ylab="Close")
points(test_df$Close[outliers_test] ~ test_df$CPI_Common[outliers_test], col="blue", pch=19)
plot(test_df$Close ~ test_df$Real_GDP, main="Real GDP vs Close", xlab="Real GDP", ylab="Close")
points(test_df$Close[outliers_test] ~ test_df$Real_GDP[outliers_test], col="blue", pch=19)
mtext("Outliers", outer=TRUE, cex=1.2)

# Influential Points
# Find the cooks distance and compare to cutoff
DCutOff_Train <- qf(0.5, length(coef(final_mod)) - 1, nrow(train_df) - length(coef(final_mod)) - 2)
D_pts <- which(cooks.distance(final_mod) > DCutOff_Train)
DCutOff_Test <- qf(0.5, length(coef(test_mod)) - 1, nrow(test_df) - length(coef(test_mod)) - 2)
D_pts_test <- which(cooks.distance(test_mod) > DCutOff_Test)
# Find DEFFITS and compare to cutoff
DFFITSCutOff <- 2 * sqrt((length(coef(final_mod)) - 1)/nrow(train_df))
DFFITS_pts <- which(abs(dffits(final_mod)) > DFFITSCutOff)
DFFITSCutOff_Test <- 2 * sqrt((length(coef(test_mod)) - 1)/nrow(test_df))
DFFITS_pts_test <- which(abs(dffits(test_mod)) > DFFITSCutOff_Test)
# Find DFBETAS and compare to cutoff
DFBETACutOff <- 2 / sqrt(nrow(train_df))
dfb_Intercept <- which(abs(dfbetas(final_mod)[,1]) > DFBETACutOff)
dfb_Volume <- which(abs(dfbetas(final_mod)[,2]) > DFBETACutOff)
dfb_CPI_Common <- which(abs(dfbetas(final_mod)[,3]) > DFBETACutOff)
dfb_Real_GDP <- which(abs(dfbetas(final_mod)[,4]) > DFBETACutOff)

DFBETACutOff_Test <- 2 / sqrt(nrow(test_df))
dfb_Intercept_test <- which(abs(dfbetas(test_mod)[,1]) > DFBETACutOff_Test)
dfb_Volume_test <- which(abs(dfbetas(test_mod)[,2]) > DFBETACutOff_Test)
dfb_CPI_Common_test <- which(abs(dfbetas(test_mod)[,3]) > DFBETACutOff_Test)
dfb_Real_GDP_test <- which(abs(dfbetas(test_mod)[,4]) > DFBETACutOff_Test)
# Get the unique points
dfb_pts <- unique(c(dfb_CPI_Common, dfb_Intercept, dfb_Real_GDP, dfb_Intercept))
influential_pts <- unique(dfb_pts, D_pts)

dfb_pts_test <- unique(c(dfb_CPI_Common_test, dfb_Intercept_test, dfb_Real_GDP_test, dfb_Intercept_test))
influential_pts_test <- unique(dfb_pts_test, D_pts_test)

# Plot in graph
# Training Set
plot(train_df$Close ~ train_df$Volume, main="Volume vs Close", xlab="Volume", ylab="Close")
points(train_df$Close[influential_pts] ~ train_df$Volume[influential_pts], col="orange", pch=19)
plot(train_df$Close ~ train_df$CPI_Common, main="CPI Common vs Close", xlab="CPI Common", ylab="Close")
points(train_df$Close[influential_pts] ~ train_df$CPI_Common[influential_pts], col="orange", pch=19)
plot(train_df$Close ~ train_df$Real_GDP, main="Real GDP vs Close", xlab="Real GDP", ylab="Close")
points(train_df$Close[influential_pts] ~ train_df$Real_GDP[influential_pts], col="orange", pch=19)
# Testing Set
plot(test_df$Close ~ test_df$Volume, main="Volume vs Close", xlab="Volume", ylab="Close")
points(test_df$Close[influential_pts_test] ~ test_df$Volume[influential_pts_test], col="orange", pch=19)
plot(test_df$Close ~ test_df$CPI_Common, main="CPI Common vs Close", xlab="CPI Common", ylab="Close")
points(test_df$Close[influential_pts_test] ~ test_df$CPI_Common[influential_pts_test], col="orange", pch=19)
plot(test_df$Close ~ test_df$Real_GDP, main="Real GDP vs Close", xlab="Real GDP", ylab="Close")
points(test_df$Close[influential_pts_test] ~ test_df$Real_GDP[influential_pts_test], col="orange", pch=19)
mtext("Influential Points", outer=TRUE, cex=1.2)
```

Relatively same amount of influential/leverage points and outliers :)

### Model Violations

```{r, fig.height=5.5, fig.width=14}
# Condition 1
par(mfrow=c(1,2))
# Training Set
plot(train_df$Close ~ fitted(final_mod), main="Y versus Y-hat", xlab="Y-hat", ylab="Y")
abline(a=0, b=1)
lines(lowess(train_df$Close ~ fitted(final_mod)), lty=2)
# Testing Set
plot(test_df$Close ~ fitted(test_mod), main="Y versus Y-hat", xlab="Y-hat", ylab="Y")
abline(a=0, b=1)
lines(lowess(test_df$Close ~ fitted(test_mod)), lty=2)
```

```{r, fig.height=5.5, fig.width=7}
# Condition 2
pairs(train_df[, c(2, 4, 7)], lower.panel=NULL)
pairs(test_df[, c(2, 4, 7)], lower.panel=NULL)
```

### Residual Plots
```{r, fig.height=13.75, fig.width=7}
# Get the residuals from spec 1
final_r <- rstandard(final_mod)
test_r <- rstandard(test_mod)
# Plot the residual plots
par(mfrow=c(5,2))
# Residual plot for the fitted values
plot(final_r ~ fitted(final_mod), xlab="Fitted Values", ylab="Residual", main="Fitted Values vs Residual for Training Set")
abline(a = 0, b = 0)
plot(test_r ~ fitted(test_mod), xlab="Fitted Values", ylab="Residual", main="Fitted Values vs Residual for Testing Set")
abline(a = 0, b = 0)
# Residual plots for the predictors
for (i in c(2,4,7)) {
  plot(final_r ~ train_df[, i], xlab=gsub("_", " ", colnames(train_df)[i]), ylab="Residual", main=paste(gsub("_", " ", colnames(train_df)[i]), "vs Residual in Training Set"))
  abline(a = 0, b = 0)
  plot(test_r ~ test_df[, i], xlab=gsub("_", " ", colnames(test_df)[i]), ylab="Residual", main=paste(gsub("_", " ", colnames(test_df)[i]), "vs Residual in Testing Set"))
  abline(a = 0, b = 0)
}

# Plot the qq plots
qqnorm(final_r)
qqline(final_r)
qqnorm(test_r)
qqline(test_r)
```

### EDAs
```{r, fig.width=7, fig.height=5.5}
par(mfrow=c(2,2))
for (i in c(1,2,4,7)) {
  hist(
    train_df[,i],
    main=paste("Histogram of", gsub("_", " ", names(train_df)[i]), "in Training Set"),
    xlab=gsub("_", " ", names(train_df)[i])
    )
  hist(
    test_df[,i],
    main=paste("Histogram of", gsub("_", " ", names(test_df)[i]), "in Testing Set"),
    xlab=gsub("_", " ", names(test_df)[i])
    )
}
```

```{r, fig.width=7, fig.height=8.25}
# Plot scatterplots against Close
par(mfrow=c(3,2))
for (i in c(2,4,7)) {
  plot(
    train_df[,i],
    train_df$Close,
    main=paste(gsub("_", " ", names(train_df)[i]), "vs", "Close", "in Training Set"),
    xlab=gsub("_", " ", names(train_df)[i]),
    ylab="Close"
    )
  plot(
    test_df[,i],
    test_df$Close,
    main=paste(gsub("_", " ", names(test_df)[i]), "vs", "Close", "in Testing Set"),
    xlab=gsub("_", " ", names(test_df)[i]),
    ylab="Close"
    )
}
```

EDAs all good reasonable :)

### Model Coefficients
```{r}
summary(final_mod)
summary(test_mod)
```


The huge discrepencies may be due to the fact that the standard deviations for Volume and Real GDP varies greatly between the two sets of data. For volume, the difference is roughly 1 billion shares; for real GDP, it the difference in standard deviation is roughly 10 billion US dollars. So we are unable to verify our model due to the huge discrepancies between the variations in our samples.

